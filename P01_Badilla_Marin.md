# PY01 Restaurantes e2
__Curso:__ Bases de Datos II  
__Periodo:__ I Sem 2025  
__Estudiantes:__ Brandon Badilla y Josi Marin  
__Fecha de entrega:__ 17 de mayo del 2025  

## Video
https://youtu.be/_1SNJjjM_tg 

## GitHub


## Contenidos

- Arquitectura L√≥gica p.2
- Arquitectura F√≠sica p.3
- Flujo de Datos p.5
- Microservicios 
  - API p.6
  - B√∫squeda p.7
- Otros componentes
  - Cach√© p.13
  - Bases de datos simult√°neas p.16
  - Balanceador de carga/router p.18
  - Pipeline CI/CD p.19



## Ejecucion

#### Requisitos Previos
 - Tener instalado Docker. De no cumplirlo, acceder [aqui](https://docs.docker.com/desktop/setup/install/windows-install/) para descargarlo.
 - Tener Docker correctamente configurado para ejecucion de proyectos. De no cumplirlo, acceder [aqui](https://docs.docker.com/get-started/) para realizar los pasos de configuracion.
 - Descargar el codigo fuente del proyecto.
 - (Opcional) Tener instalado Visual Studio Code. De no cumplirlo, puede ser necesario cambiar los comandos de ejecuci√≥n ligeramente. Puede acceder [aqui](https://code.visualstudio.com/docs/setup/windows)

#### Ejecucion
Los siguientes pasos de ejecucion son para la terminal Powershell Windows dentro del editor de texto Visual Studio Code. 

1. __Descargar imagenes de Docker__
Con los siguientes comandos se descargar√°n las versiones m√°s recientes de las imagenes creadas para este proyecto. 

```powershell
docker pull josianamj/postgres_db:latest 
docker pull josianamj/backend_image:latest
docker pull josianamj/redis_image:latest
docker pull josianamj/setup_mongo:latest
```



2. __Levantar los contenedores__
Por medio del siguiente comando, se realizar√° el levantamiento de los contenedores que componen el sistema del proyecto.

```powershell
docker-compose up -d
```

‚ö†Ô∏è __Importante:__ Al ejecutar este comando, el servicio toma un tiempo en levantarse completamente. No realizar consultas hasta que el comando "termine" su ejecucion pues resultara en errores. 

Si desea realizar escalado vertical de los microservicios, puede agregar el argumento `--scale service_name=quantity` al comando previo, por ejemplo, levantar 3 instancias de la API se veria de la siguiente forma:
```powershell
docker compose up -d --scale backend=3 
```

3. __Bajar los contenedores:__ Al finalizar las pruebas, puede "bajar" (detener la ejecucion y eliminar) los contenedores por medio del siguiente comando:

```powershell
docker compose down -v
```



# **Arquitectura L√≥gica**

En esta secci√≥n, describimos como est√°n organizados los contenedores a nivel funcional. 


## Componentes

| Componente       | Descripci√≥n                                                                                                                   |
|------------------|-------------------------------------------------------------------------------------------------------------------------------|
| **backend**       | Microservicio principal (API) para consultas del sistema, implementado en Node.js.                                           |
| **search**        | Servicio de b√∫squeda avanzada usando ElasticSearch para consultas complejas.                                                 |
| **bases de datos**| Se utilizan PostgreSQL (relacional) y MongoDB (sharding + replicaci√≥n), no de forma simult√°nea.                              |
| **cach√©**         | Servicio Redis para almacenamiento de respuestas frecuentes y mejora del rendimiento.                                        |
| **balanceador**   | Traefik como proxy inverso y balanceador de carga en ambientes con escalamiento horizontal.                                  |
| **pipeline**      | Pipeline CI/CD en GitHub Actions para pruebas autom√°ticas y publicaci√≥n de im√°genes en Docker Hub (`josianamj`).                      |



## Diagrama

![Logs de backend](./img/Infraestructura.png)

# **Arquitectura F√≠sica**

Esta secci√≥n describe c√≥mo se implementa f√≠sicamente la arquitectura del sistema de microservicios utilizando contenedores Docker, redes virtuales y vol√∫menes persistentes.

## Infraestructura de contenedores

Todo el sistema est√° orquestado mediante `docker-compose`, lo que permite levantar m√∫ltiples servicios de forma coordinada. Cada componente del sistema corre como un contenedor independiente y aislado.

### Contenedores principales

| Componente     | Imagen                           | Rol                                                       |
|----------------|----------------------------------|------------------------------------------------------------|
| `backend`      | `josianamj/backend_image`        | API principal que interact√∫a con bases de datos y clientes |
| `search`       | `josianamj/search_service`       | Servicio para consultas distribuidas en MongoDB            |
| `db`           | `josianamj/postgres_db`          | Base de datos PostgreSQL                                   |
| `redis`        | `josianamj/redis_image`          | Sistema de cach√© en memoria                                |
| `traefik`      | `traefik:v2.11`                  | Proxy inverso y balanceador de carga                       |
| `mongos1`      | `mongo`                          | Enrutador del cl√∫ster MongoDB                              |
| `mongors*`     | `mongo`                          | R√©plicas para los shards de MongoDB                        |
| `mongocfg*`    | `mongo`                          | Config servers de MongoDB                                  |
| `setup-mongo`  | `josianamj/setup_mongo`          | Script de configuraci√≥n inicial del cl√∫ster MongoDB        |
| `charge-data`  | `josianamj/charge_data`          | Script de insercion de datos simulados utilizando MML      |


## Redes

Se definieron redes personalizadas para aislar el tr√°fico seg√∫n los contextos funcionales:

- `backend-network`: conecta `traefik`, `backend` y `db`
- `mongo-replica-sharding`: conecta todos los nodos Mongo, `search`, y `redis`

Estas redes aseguran que los servicios s√≥lo puedan comunicarse con aquellos a los que est√°n destinados.


## Puertos expuestos

| Servicio  | Puerto externo | Interno en contenedor |
| --------- | -------------- | --------------------- |
| `traefik` | `80`, `8080`   | `80`, `8080`          |
| `backend` | (interno solo) | `3000`                |
| `search`  | (interno solo) | `4000`                |
| `db`      | `5432`         | `5432`                |
| `redis`   | `6379`         | `6379`                |
| `mongos1` | `27019`        | `27017`               |

A excepcion de Traefik, el resto de servicios no exponen sus puertos publicamente, solo se accede a ellos mediante redes internas de Docker.
A Traefik se accede por medio de http://localhost/

## Escalabilidad

El servicio `backend` se pueden escalar horizontalmente mediante:
```powershell
docker compose up -d --scale backend=3
```
A lo cual Traefik detecta automaticamente las nuevas instancias y balancea la carga de las solicitudes entre ellas.

## Persistencia

El sistema esta dise√±ado para asegurar la persistencia de datos mediante vol√∫menes de Docker, en caso de que un contenedor reinicie o entre m√∫ltiples ejecuciones.

El sistema est√° dise√±ado para asegurar la persistencia de los datos cr√≠ticos mediante el uso de **vol√∫menes Docker**. Estos vol√∫menes permiten que los datos almacenados por los contenedores no se pierdan al reiniciar o eliminar los servicios.

| Servicio            | Volumen asociado     | Ruta interna                     | Prop√≥sito principal                                 |
|---------------------|----------------------|----------------------------------|-----------------------------------------------------|
| `db` (PostgreSQL)   | `db-data`            | `/var/lib/postgresql/data`       | Almacena datos estructurados del sistema            |
| `elasticsearch`     | `elastic-data`       | `/usr/share/elasticsearch/data`  | Persistencia del √≠ndice de b√∫squeda                 |
| `mongors*`          | `data1` a `data9`    | `/data/db`                       | Persistencia por r√©plica y shard                    |
| `mongocfg*`         | `config1` a `config3`| `/data/db`                       | Datos de configuraci√≥n del cl√∫ster MongoDB          |



# **Flujo de Datos**

En esta secci√≥n, describimos c√≥mo fluyen los datos desde que el cliente realiza una consulta hasta que recibe su respuesta.

En las rutas de los ejemplos, se reemplaza la finalizacion de una consulta espec√≠fica por __*__ para generalizar.

![Flujo Datos](./img/flujoDatos.png)

## Flujo de una consulta simple
1. El __cliente__ hace petici√≥n a `http://localhost/api/*`
2. __Traefik__ enruta hacia una instancia del __backend__.
3. __Backend__ consulta a __Redis__ si esta respuesta est√° en cach√©.
4. Si s√≠, se considera un _cach√© hit_ y Redis devuelve la respuesta a __Backend__.
5. Si no, se considera un _cach√© miss_. __Backend__ consulta a la __Database__ correspondiente, sea este PostgreSQL o MongoDB.
6. __Database__ env√≠a la informaci√≥n a __Backend__.
7. __Backend__ responde al __cliente__. 


# **API REST - Sistema de Gesti√≥n de Restaurante**

Esta API est√° dise√±ada para gestionar un sistema de restaurante, incluyendo men√∫s, pedidos, reservas, restaurantes y usuarios. Est√° implementada con Node.js, Express, PostgreSQL/MongoDB y Redis para optimizaci√≥n de cach√©.


## **Endpoints** / Rutas

### Men√∫s
- `GET /api/menus` - Obtener todos los men√∫s
- `GET /api/menus/:id` - Obtener un men√∫ por ID
- `POST /api/menus` - Crear un nuevo men√∫
- `PUT /api/menus/:id` - Actualizar un men√∫ existente
- `DELETE /api/menus/:id` - Eliminar un men√∫


### Pedidos (Orders)
- `GET /api/orders` - Obtener todos los pedidos
- `GET /api/orders/:id` - Obtener un pedido por ID
- `POST /api/orders` - Crear un nuevo pedido
- `PUT /api/orders/:id` - Actualizar un pedido existente
- `DELETE /api/orders/:id` - Eliminar un pedido


### Reservas (Reservations)
- `GET /api/reservations` - Obtener todas las reservas
- `GET /api/reservations/:id` - Obtener una reserva por ID
- `POST /api/reservations` - Crear una nueva reserva
- `PUT /api/reservations/:id` - Actualizar una reserva existente
- `DELETE /api/reservations/:id` - Eliminar una reserva


### Restaurantes (Restaurants)
- `GET /api/restaurant` - Obtener todos los restaurantes
- `GET /api/restaurant/:id` - Obtener un restaurante por ID
- `POST /api/restaurant` - Crear un nuevo restaurante
- `PUT /api/restaurant/:id` - Actualizar un restaurante existente
- `DELETE /api/restaurant/:id` - Eliminar un restaurante



### Usuarios (Users)
- `GET /api/user` - Obtener todos los usuarios
- `GET /api/user/:id` - Obtener un usuario por ID
- `POST /api/user` - Crear un nuevo usuario
- `PUT /api/user/:id` - Actualizar un usuario existente
- `DELETE /api/user/:id` - Eliminar un usuario


### Products (Productos)
- `GET /api/products` - Obtener todos los usuarios
- `GET /api/product/:id` - Obtener un usuario por ID
- `POST /api/product` - Crear un nuevo usuario
- `PUT /api/product/:id` - Actualizar un usuario existente
- `DELETE /api/product/:id` - Eliminar un usuario


## **Middleware de Cach√©**
Se implementa un sistema de cach√© en Redis para optimizar las operaciones `GET` en cada endpoint. El middleware verifica si los datos ya est√°n en cach√©:
- Si existe: Se devuelve desde Redis.
- Si no existe: Se consulta en la base de datos y se almacena en Redis para la siguiente petici√≥n.

## Patron Repositorio

Previamente en la TC01 se tenia el manejo de la base de datos desde la API utilizando Modelo, Ruta y Controlador por cada tabla. Sin embargo, para la implementacion de este proyecto es necesario usar de manera intercambiable la base de datos postgres y el uso de mongo, por lo cual se tuvo que reemplazar los _Modelos_ por _Repositorios_. 

El patr√≥n Repositorio permite mantener una estructura centralizada donde se registran las distintas implementaciones de repositorios, y se selecciona din√°micamente cu√°l usar en tiempo de ejecuci√≥n. Por tanto, se desencopla totalemente la logica del negocio de la logica de las implementaciones de bases de datos. 

La estructura de esta implementacion se observa de la siguiente manera:

```bash
repositories/
‚îú‚îÄ‚îÄ mongo/
‚îÇ ‚îú‚îÄ‚îÄ menuRepository.js
‚îÇ ‚îú‚îÄ‚îÄ orderRepository.js
‚îÇ ‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ postgres/
‚îÇ ‚îú‚îÄ‚îÄ menuRepository.js
‚îÇ ‚îú‚îÄ‚îÄ orderRepository.js
‚îÇ ‚îú‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ repositoryFactory.js
```



# **Integraci√≥n de ElasticSearch en proyecto restaurante**

Este documento detalla los intentos realizados para integrar ElasticSearch en el proyecto del restaurante, con el objetivo de indexar productos en un motor de b√∫squeda r√°pido y optimizado. Aunque el resultado no fue exitoso, se documenta el proceso, los errores encontrados y las soluciones intentadas.


## **Propuesta de implementaci√≥n**

Dentro del repositorio se a√±adieron los siguientes archivos: 

```yaml
backend/
‚îÇ
‚îú‚îÄ‚îÄ db/
‚îÇ   ‚îú‚îÄ‚îÄ elasticClient.js               # Cliente de conexi√≥n a Elastic
‚îÇ   ‚îú‚îÄ‚îÄ createElasticIndex.js          # Creaci√≥n del √≠ndice en Elastic
‚îÇ
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îî‚îÄ‚îÄ searchController.js            # L√≥gica para b√∫squedas
‚îÇ
‚îú‚îÄ‚îÄ repositories/
‚îÇ   ‚îî‚îÄ‚îÄ searchRepository.js            # M√©todos para interactuar con ElasticSearch
‚îÇ
‚îú‚îÄ‚îÄ routes/
‚îÇ   ‚îî‚îÄ‚îÄ searchRoutes.js                # Rutas para b√∫squedas
‚îÇ
‚îî‚îÄ‚îÄ app.js 
```

## **Levantar el servicio en el compose**
```yaml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
  environment:
    - discovery.type=single-node
    - network.host=0.0.0.0
    - xpack.security.enabled=false
  ports:
    - "9200:9200"
  volumes:
    - elastic-data:/usr/share/elasticsearch/data
  networks:
    - backend-network
```

## **Crear el cliente de conexi√≥n**
```Javascript
const { Client } = require('@elastic/elasticsearch');

const elasticClient = new Client({
  node: process.env.ELASTIC_URL,
  maxRetries: 5,
  requestTimeout: 60000,
  sniffOnStart: true
});

module.exports = elasticClient;
```

## Prueba de las conexiones 

Este log muestra la conexi√≥n con el servicio de elastic que se levant√≥ en el compose

![Logs de backend](./img/elastciBack.jpg)

Para confirmar que ElasticSearch est√© en pie y respondiendo correctamente desde el backend, se hizo una prueba interna en el contendedor:

```Javascript
(async () => {
  try {
    await client.ping({}, { requestTimeout: 3000 });
    console.log('üü¢ Elasticsearch is up and running!');
  } catch (error) {
    console.error('üî¥ Elasticsearch cluster is down!', error.message);
  }
})();
```

Dichaprueba devolvi√≥ el resultado esperado:

![Logs de backend](./img/logBackend.jpg)


## ‚ö†Ô∏è**Errores a partir de aqu√≠**

1. **Creaci√≥n del index de Products**

Se cre√≥ un archivo llamado createElasticIndex.js cuya funci√≥n era al hacer node al app.js levantar el √≠ndice "products" para guardar los productos, sin embargo, al hacer el compose y revisar los logs del backend se mostraba el mensaje de error en la creaci√≥n.

![Logs de backend](./img/indexError.jpg)

Lo que significa que el archivo `createElasticIndex.js` no estaba levantando el √≠ndice de productos.

Para solucionar esto, entramos al contenedor directamente para crear el √≠ndice desde ah√≠.

##### Creaci√≥n manual del √≠ndice

Para la creaci√≥n manual entramos al contenedor  con el comando docker exec -it proyecto-1-backend-1 /bin/bash `docker exec -it proyecto-1-backend-1 /bin/bash`

Y desde le contenedor se hizo la solicitud 

```bash
curl -X PUT "http://elasticsearch:9200/products" -H 'Content-Type: application/json' -d '{
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "description": { "type": "text" },
      "menu_id": { "type": "integer" },
      "price": { "type": "float" }
    }
  }
}'
```
Luego de crearlo se verifica la existencia del mismo con el comando:  `curl -X GET "http://elasticsearch:9200/_cat/indices?v"`

El resultado fue el siguiente:

![Logs de backend](./img/manualIndex.jpg)

En donde se ve que el √≠ndice fue creado.

A partir de aqu√≠ se modific√≥ el controller de productos para insertar una vez creado un producto en las bases de datos y en Elastic porque logrando insertar los productos, la b√∫squeda ser√≠a m√°s rapida de implementar.

Pero los problemas a partir del intento de insertar productos no nos dejaron avanzar.

Cuando se intent√≥ hacer un POST para crear un producto se obtuvo el siguiente error:

![POST product postman error](./img/versionCrashed.jpg)

Ese error se debe a un problema de compatibilidad entre la versi√≥n de Elasticsearch (8.x) y la configuraci√≥n de los headers en el cliente de Elasticsearch.

Para solucionarlo tratamos de cambiar la imagen del compose a latest, pero eso tambi√©n nos dio un error:

![Latest error](./img/latestError.jpg)

Ese error se corrigi√≥ arreglando el compose y la versi√≥n d ela imegn, sin embargo, apareci√≥ un nuevo error al tratar de hacer el POST:


ResponseError: media_type_header_exception<br>	Caused by:<br>		illegal_argument_exception: Incorrect header [Accept]. Only one value should be provided<br>	Root causes:<br>		media_type_header_exception: Invalid media-type value on headers [Accept]<br> &nbsp; &nbsp;at SniffingTransport._request (/app/node_modules/@elastic/transport/lib/Transport.js:533:27)<br> &nbsp; &nbsp;at process.processTicksAndRejections (node:internal/process/task_queues:95:5)<br> &nbsp; &nbsp;at async /app/node_modules/@elastic/transport/lib/Transport.js:631:32<br> &nbsp; &nbsp;at async SniffingTransport.request (/app/node_modules/@elastic/transport/lib/Transport.js:627:20)<br> &nbsp; &nbsp;at async Client.IndexApi [as index] (/app/node_modules/@elastic/elasticsearch/lib/api/api/index.js:77:12)<br> &nbsp; &nbsp;at async createProduct (/app/controllers/productController.js:28:5)</pre>


Este error est√° relacionado a la forma en que las versiones manejan los headers en los request. Despu√©s de aqu√≠ no pudimos solucionar y comenzamos a tener problemas con la conexion con Elastic y con el network del backend.

![Server error](./img/serverLost.jpg)

![Network error](./img/networkLost.jpg)

## Conclusiones

Se realizaron m√∫ltiples intentos de conexi√≥n y configuraci√≥n para que ElasticSearch funcionara correctamente con Docker, Traefik y Node.js.

Los problemas de incompatibilidad de headers y errores de red fueron las causas principales de los fallos.

A pesar de los m√∫ltiples intentos, la integraci√≥n no se complet√≥ exitosamente, pero queda demostrado en la documentaci√≥n los errores y las soluciones probadas.



# **Implementaci√≥n de Cach√© con Redis en API REST**

## **Descripci√≥n General**
La integraci√≥n de Redis en nuestra API REST permite optimizar el rendimiento de las consultas a la base de datos PostgreSQL o Mongo. Mediante el uso de un middleware de cach√©, las respuestas frecuentes se almacenan temporalmente en Redis, reduciendo el tiempo de respuesta en futuras solicitudes.


## **Infraestructura en Docker Compose**
La configuraci√≥n para Redis se define en el archivo `docker-compose.yml`, el cual especifica un contenedor dedicado para Redis:

```yaml
redis:
  container_name: redis
  image: redis:latest
  ports:
    - "6379:6379"
  networks:
    - backend-network
```


## **Conexi√≥n a Redis en el Proyecto**
La conexi√≥n a Redis se realiza mediante un cliente de Redis en un archivo especifico:

```javascript
// db/redisClient.js
const redis = require("redis");
const client = redis.createClient({
  url: process.env.REDIS_URL,
});
const DB_TYPE = process.env.DB_TYPE;

client.on("connect", () => {
  console.log("Redis conectado exitosamente");
});
client.on("error", (err) => {
  console.error("Error en Redis:", err);
});
(async () => {
  await client.connect();
})();
const getPrefixedKey = (key) => `${DB_TYPE}:${key}`;
module.exports = {
  client,
  getPrefixedKey,
};
```



## **Middleware de Cach√©**
Para evitar duplicaci√≥n de c√≥digo, se desarroll√≥ un middleware reutilizable, esto para que en cada controlador del backend sea m√°s unificado y evite duplicaci√≥n de trabajo.

```javascript
// middlewares/cacheMiddleware.js
const { client: redisClient, getPrefixedKey } = require("../db/redisClient");

const cacheMiddleware = (keyGenerator) => async (req, res, next) => {
  const cacheKey = getPrefixedKey(keyGenerator(req));

  try {
    const cachedData = await redisClient.get(cacheKey);

    if (cachedData) {
      console.log("Cache hit");
      return res.status(200).json({
        status: 200,
        message: "Data fetched from cache",
        data: JSON.parse(cachedData),
      });
    }

    console.log("Cache miss");
    next();
  } catch (err) {
    console.error("Error al acceder al cach√©:", err);
    next();
  }
};

module.exports = cacheMiddleware;
```

##  **Integraci√≥n en Controladores**
A modo de ejemplo, vamos a integrar el cach√© en el controlador de menus.
En el controlador de men√∫s, el cach√© se implementa de la siguiente forma:
Es en el controlador donde se manjea la pol√≠tica de expiraci√≥n que en nuestro caso es 1 hora.

```javascript
// controllers/menuController.js
const { getRepository } = require("../repositories/repositoryFactory");
const menuRepo = getRepository("menu");
const { client: redisClient, getPrefixedKey } = require("../db/redisClient");

const getAllMenus = async (req, res, next) => {
  try {
    const menus = await menuRepo.findAll();
    res.status(200).json({
      status: 200,
      message: "Menus fetched successfully",
      data: menus,
    });

    // Guardar en cach√©
    const cacheKey = getPrefixedKey("all_menus");
    await redisClient.set(cacheKey, JSON.stringify(menus), {
      EX: 60 * 60, // Expira en 1 hora
    });
  } catch (err) {
    console.error("Error en getAllMenus:", err.message);
    next(err);
  }
};

module.exports = {
  getAllMenus,
};
```



##  **Resultado Esperado**
- Primera consulta a `/api/menus` ‚Üí **Cache Miss** y respuesta desde PostgreSQL/Mongo.
- Segunda consulta ‚Üí **Cache Hit** y respuesta desde Redis.
- Clave almacenada en Redis: `postgres:all_menus`.

Para validar en Redis:
```bash
docker exec -it redis redis-cli
127.0.0.1:6379> KEYS *
```

Con eso se ven todas las keys que han sido guardadas hasta el momento.



# **Bases de Datos en el Sistema**

En esta seccion detallamos la arquitectura de bases de datos h√≠brida que utiliza nuestro sistema, integrando tanto una base relacional (PostgreSQL) como una base NoSQL distribuida (MongoDB con sharding y replicaci√≥n).

### Configuracion
Los controladores y modelos en el c√≥digo se dise√±aron con el patr√≥n Repositorio para permitir el intercambio entre __PostgreSQL__ y __MongoDB__ mediante la variable de configuraci√≥n _DB_TYPE_.


## üêò PostgreSQL

Esta es la base de datos principal, generada previamente en el Trabajo Corto #1. 

![Postgres](./img/postgres.png)

## üçÉ MongoDB con Replicaci√≥n y Sharding

Esta base de datos se implemento como parte del Trabajo Corto #3, con la funci√≥n de ser reutilizada en este proyecto. 
Se parte de c√≥digo brindado por el docente, al cu√°l se le agrega un shard y r√©plica adicional para cumplir las especificaciones del proyecto. Adem√°s, se configuran los datos, particiones y otros. 

![Diagrama Traefik](./img/mongo.png)

#### Arquitectura configurada

- __Conjunto de r√©plicas:__ cada shard contiene 3 r√©plicas (1 primario y 2 secundarios) para alta disponibilidad.
-  __Sharding:__ los datos de productos y reservas se distribuyen en m√∫ltiples shards para soportar grandes vol√∫menes y paralelismo. 
   - La coleccion _Usuarios_ se configura con _Hashed Sharding_ con la llave _UserId_.
   - Las colecciones _Publicaciones_ y _Comentarios_ se configuran con _Ranged Sharding_, utilizando las llaves _UserId_ y _PostId_ consecutivamente. 
- __Persistencia:__ Todos los contenedores Mongo usan vol√∫menes Docker dedicados para almacenar los datos y garantizar la persistencia en reinicios o actualizaciones. Si se iniciar desde 0, es importante eliminar los volumenes.

#### Componentes del cl√∫ster MongoDB

- `mongors1n1`, `mongors1n2`, `mongors1n3` ‚Üí R√©plicas del shard 1
- `mongors2n1`, `mongors2n2`, `mongors2n3` ‚Üí R√©plicas del shard 2
- `mongors3n1`, `mongors3n2`, `mongors3n3` ‚Üí R√©plicas del shard 3
- `mongocfg1`, `mongocfg2`, `mongocfg3` ‚Üí Config servers
- `mongos1` ‚Üí Enrutador del cl√∫ster




# **Uso del Balanceador de Carga**

### Funci√≥n

En nuestra arquitectura, generamos un contenedor con el servicio Traefik para implementar un balanceador de carga y proxy inverso que redirige el tr√°fico a los servicios correctos seg√∫n la ruta utilizada en una consulta espec√≠fica.  

![Diagrama Traefik](./img/traefik.png)


### Configuraci√≥n

Decidimos utilizar __Traefik__ sobre otros servicios de balanceo de carga como Nginx o Kubernetes debido a su facilidad de configuraci√≥n mediante _labels_ en el _docker_compose_ y el balanceo de carga autom√°tico.

#### Rutas configuradas

- `/api/**` ‚Üí servicio __Backend__.
- `/search/**` ‚Üí servicio __Elastic__.

#### Configuraci√≥n
- __Red:__ para la comunicaci√≥n de __Traefik__ con los servicios __Backend__ y __Elastic__ se necesitan agregar a una red com√∫n, para lo cual se eligi√≥ __backend-network__.
- __Puertos:__ el servicio __Traefik__ est√° escuchando en el puerto _8080_, el cual es el default de _localhost_. Adem√°s, los servicios __Backend__ y __Elastic__ se configuraron para escuchar a __Traefik__ en los puertos _3000_ y _4000_ consecutivamente. 
- __Verificacion:__ gracias al argumento `--api.insecure=true` se logra tener acceso al Dashboard propio del servicio en la direcci√≥n http://localhost:8080/dashboard/.
- __Balanceo:__ el servicio de balanceo de cargas de __Traefik__ balancea entre consultas en multiples instancias de un servicio, por lo cual si no se escala un microservicio no se activa ni es necesario el balanceo. La estrategia de balanceo utilizada es la predeterminada: RoundRobin que toma en cuenta la carga actual en cada instancia de un servicio. 


### Referencias utilizadas
Para esto, nos basamos de tutoriales en YouTube y la documentaci√≥n de la p√°gina [Traefik & Docker](https://doc.traefik.io/traefik/routing/providers/docker/).



# **Pipeline CI/CD**

Debido a que de previo estabamos utilizando _GitHub_ como servicio de control de versiones Git, fue instintivo utilizar __Github Actions__ como herramienta de integracion despliegue continuo. 

### Configuracion 

La configuracion del pipeline CI/CD se realizo por medio del archivo __.github\workflows\docker-image.yml__, el cual se ejecuta automaticamente en GitHub tras una actualizacion del repositorio en la rama principal (lo que se realiza con el comando `git push origin main`).

### Etapas del pipeline

1. **Instalaci√≥n de dependencias:** debido a que las pruebas unitarias y de integracion se realizaron con la libreria _Jest_ para _Node.js_, se deben instalar las dependencias necesarias para su correcta ejecucion. Las dependencias que se instalan son las mismas del microservicio __API__.
2. **Ejecuci√≥n de pruebas (unitarias/integraci√≥n):** las pruebas se ejecutan por medio del comando `npm test`. Con solo una prueba que est√© incorrecta, se interrupte el flujo del pipeline y da un s√≠mbolo de error en GitHub.
3. **Construir y subir las im√°genes Docker:** se inicia sesion en la cuenta _josianamj_ de Docker para subir las imagenes creadas para este proyecto, con la funcion de que estas puedan ser accedidas desde multiples maquinas sin necesidad de construccion. 

Si el pipeline completo se ejecuta de manera correcta, en el repositorio aparece un ‚úÖ, al hacerle click podemos ver un mensaje de √©xito.

![Pipelinr](./img/pipeline-success.png)

### Posterior al pipeline
Como indicado en el enunciado, el despliegue del sistema se realiza de forma local. Para esto, es necesario seguir los siguientes pasos:

1. __Descargar imagenes de Docker__
Con los siguientes comandos se descargar√°n las versiones m√°s recientes de las imagenes creadas para este proyecto. 

```powershell
docker pull josianamj/postgres_db:latest 
docker pull josianamj/backend_image:latest
docker pull josianamj/redis_image:latest
docker pull josianamj/setup_mongo:latest
docker pull josianamj/charge-data:latest
```

2. __Levantar los contenedores__
Por medio del siguiente comando, se realizar√° el levantamiento de los contenedores que componen el sistema del proyecto.

```powershell
docker-compose up -d
```
O alternativamente para levantar un sistema con escalado horizontal en los microservicios:
```powershell
docker compose up -d --scale backend=3 --scale¬†elastic=2
```

3. __Bajar los contenedores:__ Al finalizar las pruebas, puede "bajar" (detener la ejecucion y eliminar) los contenedores por medio del siguiente comando:

```powershell
docker compose down -v
```
